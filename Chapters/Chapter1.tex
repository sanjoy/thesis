\chapter{Introduction}

\label{Chapter1}

\lhead{Chapter 1. \emph{Introduction}}

\begin{quote}
Programming languages are notations for describing computations to
people and to machines.  The world as we know it depends on
programming languages, because all the software running on all the
computers was written in some programming language.  But, before a
program can be run, it first must be translated into a form in which
it can be executed by a computer.

The software systems that do this translation are called
compilers.\cite{dragonbook}
\end{quote}

\section{Compilers and Program Optimization}

Most compilers in current industrial use are technically
\emph{optimizing compilers}.  They make an attempt to improve runtime
behavior of the program being compiled without changing their
semantics.

\begin{minted}[linenos, frame=lines]{c}
int main() {
  int a = 2, b = 3;
  int c = a + b;
  printf("%d\n", c);
  return 0;
}
\end{minted}

For a simple example of a compiler optimization, consider the code
shown above.  Since the value of \texttt{c} can be shown to be a
compile time constant, a compiler may transform the assignment
\texttt{c} to \texttt{int c = 5;} without changing the semantics of
the program which, barring exceptional cases, should improve runtime
performance. It may then propagate the constant to the \texttt{printf}
call and do away with the earlier \emph{dead} instructions altogether.
The code generated by \texttt{gcc -O3} confirms this:

\begin{minted}[linenos, frame=lines]{gas}
subq $8, %rsp
; c has been folded away to a direct literal, 5
movl $5, %esi
; .LCO is the location of the string "%d\n" in memory
movl $.LC0, %edi
xorl %eax, %eax
call printf
xorl %eax, %eax
addq $8, %rsp
ret
\end{minted}

Below lies a more involved example -- some compilers are sophisticated
enough to algebraically infer that as long as \texttt{i} is greater
than or equal to \texttt{0}, there is a gap of \texttt{8} elements
between \texttt{i} and \texttt{2 * i + 8}. The loop can then be safely
vectorized in blocks of \texttt{8} when compiling for a processor with
vector arithmetic units (SSE on the Intel architecture, for instance),
leading to a significant speed boost.

\begin{minted}[linenos, frame=lines]{c}
for (int i = 0; i < limit; i++) {
  array[i] = array[2 * i + 8];
}
\end{minted}

Optimizations can be broadly categorized into machine specific and
machine independent optimizations.

\subsection{Machine Independent Compiler Optimizations}

Certain kinds of compiler optimizations are expected to
\textit{always} result in desirable non-semantic changes.  For
instance, consider the following loop implemented in C:

\begin{minted}[linenos, frame=lines]{c}
void hoist(int *buffer, int size) {
  int i;
  for (i = 0; i < size; i++) {
    buffer[i] = size * size;
  }
}
\end{minted}

The above program is semantically equivalent to

\begin{minted}[linenos, frame=lines]{c}
void hoist(int *buffer, int size) {
  int i;
  int value = size * size;
  for (i = 0; i < size; i++) {
    buffer[i] = value;
  }
}
\end{minted}

and yet the second version saves $\texttt{size} - 1$ multiplications.
The only time the unoptimized version runs faster than the optimized
version is when \texttt{size} is \texttt{0}.  That too can be fixed
with a conditional:

\begin{minted}[linenos, frame=lines]{c}
void hoist(int *buffer, int size) {
  if (size != 0) {
    int i;
    int value = size * size;
    for (i = 0; i < size; i++) {
      buffer[i] = value;
    }
  }
}
\end{minted}


Such an optimization (hoisting computations out of loops when
possible) is likely to be beneficial for all architectures since its
efficacy relies only on the very reasonable assumption that since
computations take a positive amount time to complete, reducing the
number of computations overall will increase program throughput.  We
can thus assert this to fall in the category of \textit{machine
  independent} compiler optimizations.

\subsection{Machine Specific Compiler Optimizations}

Consider the following expression:

\begin{minted}[linenos, frame=lines]{c}
int expression(int a, int b) {
  return a + b * 4;
}
\end{minted}

At the outset it may seem that evaluating \texttt{expression} for some
specific values for \texttt{a} and \texttt{b} requires an addition and
a multiplication by \texttt{4} (which will then be optimized to a left
shift by \texttt{2}).  The X86 instruction set, however, has a
dedicated instruction, \texttt{lea} that can compute expressions of
the form \texttt{a + b * C} where \texttt{C} is a constant integer
with the value \texttt{1}, \texttt{2}, \texttt{4} or \texttt{8}.  This
allows us to generate the following code for the function on the
x86-64 ISA:

\begin{minted}[linenos, frame=lines]{gas}
expression:
  leal (%rdi,%rsi,4), %eax
  ret
\end{minted}

Note that this observation doesn't lead to an \textit{optimization} on
platforms that lack the \texttt{lea} instruction.  On the x86 ISA,
however, an instruction scheduler aware of the semantics of
\texttt{lea} is likely to generate code that is more efficient.

Dependence on architecture-specific details doesn't need to stop at
the ISA level.  Consider the follow extract from the AMD optimization
manual \cite[Section ~6.2]{amd64opt}:

\begin{quote}

\textbf{Optimization}

Use of a two-byte near-return can improve performance. The single-byte
near-return (opcode C3h) of the RET instruction should be used
carefully. Specifically, avoid the following two situations:

\begin{itemize}
\item Any kind of branch (either conditional or unconditional) that
  has the single-byte near-return RET instruction as its target. See
  ``Examples.''

\item A conditional branch that occurs in the code directly before the
  single-byte near-return RET instruction. See ``Examples.''
\end{itemize}

\textbf{Application}

This optimization applies to:

\begin{itemize}
\item 32-bit software
\item 64-bit software
\end{itemize}

\textbf{Rationale}

The processor is unable to apply a branch prediction to the
single-byte near-return form (opcode C3h) of the RET instruction.  The
easiest way to assure the utilization of the branch prediction
mechanism is to use a two-byte RET instruction. A two-byte RET has a
REP instruction inserted before the RET, which produces the functional
equivalent of the single-byte near-return RET instruction, but is not
affected by the prediction limitations outlined above.

\end{quote}

In fact, a lot of the crucial aspects of code generation (instruction
selection and scheduling, vectorization, register allocation) require
assumptions on the target architecture.  As we shall see, this is even
more true for optimizations aiming to improve power efficiency of
generated code.

\section{Technologies Used}

Progress in this project would have been impossible without leveraging
existing state of the art software, two of which deserve special
mention here:

\subsection{The LLVM Compiler Infrastructure}

The LLVM Project\footnote{http://llvm.org} implements a compiler
pipeline that optimizes and compiles programs written in the LLVM IR
to a variety of machine architectures.  The LLVM IR is designed to
make it easy to compile most traditional programming languages into
this intermediate representation.  The language-agnostic design (and
the success) of LLVM has since spawned a wide variety of front ends:
languages with compilers which use LLVM include Objective-C, Fortran,
Ada, Haskell, Java bytecode, Python, Ruby, ActionScript, GLSL, D, and
Rust.  Moreover, the dragonegg plugin for
GCC\footnote{http://dragonegg.llvm.org/} allows LLVM to compile any
language that has a GCC frontend.

\subsubsection{The Intermediate Representation}

To use the LLVM compiler pipeline, a source program first needs to be
compiled to the LLVM intermediate
representation\footnote{http://llvm.org/docs/LangRef.html}.  The IR
follows the idiom of dividing a function's body into basic blocks and
linking the blocks into a directed control flow graph.  Instructions
are in SSA form and are typed.

As an example, the IR corresponding to the following C function

\begin{minted}[linenos, frame=lines]{c}
int function(int *a) {
  int i;
  for (i = 0; i < 100; i++) {
    a[i] = i;
  }
}
\end{minted}

after the \texttt{mem2reg} LLVM pass (which converts stack variables
to SSA registers whenever possible) and some cleanup, will look like

\begin{minted}[linenos, frame=lines]{llvm}
define i32 @function(i32* %a) nounwind {
  br label %1

; <label>:1
  %i.0 = phi i32 [ 0, %0 ], [ %7, %6 ]
  %2 = icmp slt i32 %i.0, 100
  br i1 %2, label %3, label %8

; <label>:3
  %4 = sext i32 %i.0 to i64
  %5 = getelementptr inbounds i32* %a, i64 %4
  store i32 %i.0, i32* %5, align 4
  br label %6

; <label>:6
  %7 = add nsw i32 %i.0, 1
  br label %1

; <label>:8
  ret i32 undef
}
\end{minted}

We chose to work with LLVM because if its clean design.  LLVM has been
designed inside-out keeping modularity and ease of modification in
sight.  These are some of the factors that make LLVM a tool of choice
for conducting academic research\footnote{http://llvm.org/pubs/}.

\subsection{Pin - A Dynamic Binary Instrumentation Tool}

Pin\footnote{http://software.intel.com/en-us/articles/pin-a-dynamic-binary-instrumentation-tool},
by Intel, is a runtime instrumentation framework for the x86-64 and
IA32 ISA.  Pin allows the creation of \textit{pintools} which allow a
programmer to instrument a binary at arbitrary points with arbitrary
code.  Since it works with binaries, Pin can be used to instrument
programs without requiring access to their source code or without
recompiling them -- a welcome convenience.

\subsubsection{Working with Pin}

Pin breaks up the execution of a program into \textit{traces}, or
straight line sequence of machine instructions.  Pin runs callbacks
provided by the user-created tool (a shared object, essentially) on
this trace to generate a new instrumented trace which is then
executed.  In a way, Pin is essentially a `just in time' (JIT)
compiler, traces are recompiled into instrumented traces just before
they're executed.  To be instrumented, an instruction needs to be
executed at least once.

Pin provides a rich introspection API to reading off the state of the
processor; like the state of the register file and the last value read
off the data bus.  Such facilities were instrumental (no pun
intended!) in our decision to adopt it.

\subsubsection{An example Pintool}

Consider the simplest possible tool -- one which counts the number of
instructions executed by an executing binary (a complete tool that
does this can be found in \texttt{tools/ManualExamples/inscount0.cpp}
in the Pin distribution).  Here is how such a tool would look in
principle:

\begin{minted}[linenos, frame=lines]{cpp}
static void one_instruction_executed() {
  inst_count++;
}

static void instrument_instruction(INS ins, void *) {
  /* Insert a call to one_instruction_executed before every 
   * instruction, no arguments are passed.  */
  INS_InsertCall(ins, IPOINT_BEFORE, (AFUNPTR)one_instruction_executed, 
                  IARG_END);
}

int main(int argc, char * argv[]) {
  /* ... */
  INS_AddInstrumentFunction(instrument_instruction, 0);
  /* ... */
}
\end{minted}

